{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, cache=True)\n",
    "X/=255.\n",
    "y = y.astype(int)\n",
    "X,X_test,y,y_test = train_test_split(X,y,test_size=10000)\n",
    "\n",
    "# Extract number of data points, and the height and width of the images for later reshaping\n",
    "m = X.shape[0]\n",
    "n = X.shape[1]\n",
    "\n",
    "h = 28\n",
    "w = 28\n",
    "\n",
    "N = 10\n",
    "\n",
    "X = torch.from_numpy(X)\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y = torch.from_numpy(y)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "X = X.to(torch.float32)\n",
    "X_test = X_test.to(torch.float32)\n",
    "y = y.to(torch.long)\n",
    "y_test = y_test.to(torch.long)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X = X.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y = y.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "training_data = TensorDataset(X,y)\n",
    "test_data = TensorDataset(X_test,y_test)\n",
    "\n",
    "batch_size = 256\n",
    "train_loader = torch.utils.data.DataLoader(dataset=training_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "batch_size = 256\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 2\n",
    "h_dim_1 = 512\n",
    "h_dim_2 = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,n,latent_dim,h_dim_1,h_dim_2):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Encoder,self).__init__()\n",
    "        self.l1 = nn.Linear(n,h_dim_1) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(h_dim_1,h_dim_2)\n",
    "        self.l3_mu = nn.Linear(h_dim_2,latent_dim)\n",
    "        self.l3_rho = nn.Linear(h_dim_2,latent_dim)\n",
    "    \n",
    "    def forward(self,x,sample=True):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        a1 = self.l1(x)\n",
    "        z1 = torch.relu(a1)   \n",
    "        \n",
    "        a2 = self.l2(z1)\n",
    "        z2 = torch.relu(a2)\n",
    "        mu = self.l3_mu(z2)\n",
    "        rho = self.l3_rho(z2)\n",
    "        return mu, rho\n",
    "    \n",
    "# rho = log sigma^2\n",
    "# sigma = exp( rho/2 )\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,n,latent_dim,h_dim_1,h_dim_2):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Decoder,self).__init__()\n",
    "        self.l1 = nn.Linear(latent_dim,h_dim_1) # Transform from input to hidden layer\n",
    "        self.l2 = nn.Linear(h_dim_1,h_dim_2)\n",
    "        self.l3 = nn.Linear(h_dim_2, n)\n",
    "        \n",
    "    def forward(self,z):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "\n",
    "        a1 = self.l1(z)\n",
    "        z1 = torch.relu(a1)   \n",
    "        \n",
    "        a2 = self.l2(z1)\n",
    "        z2 = torch.relu(a2) \n",
    "        \n",
    "        a3 = self.l3(z2) \n",
    "        z3 = torch.sigmoid(a3)\n",
    "        return z3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(n,latent_dim,h_dim_1,h_dim_2)\n",
    "encoder.to(device)\n",
    "\n",
    "decoder = Decoder(n,latent_dim,h_dim_1,h_dim_2)\n",
    "decoder.to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam([e for e in encoder.parameters()]+[p for p in decoder.parameters()],lr=1e-3)\n",
    "\n",
    "epochs = 50\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    # Loop over each subset of data\n",
    "    dl = 0\n",
    "    kl = 0\n",
    "    n_batches = 0\n",
    "    for d,_ in train_loader:\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        mu, rho = encoder(d)\n",
    "        eps = torch.randn_like(rho)\n",
    "        latent = mu + torch.exp(rho/2.)*eps\n",
    "        reconstruction = decoder(latent)\n",
    "        \n",
    "        # Compute the loss\n",
    "        #data_loss = 0.5*torch.sum((reconstruction - d)**2/sigma_data**2,axis=-1)\n",
    "        data_loss = -0.5*torch.sum(d*torch.log(torch.clamp(reconstruction,min=1e-4)) + (1-d)*torch.log(torch.clamp(1-reconstruction,min=1e-4)),axis=-1)\n",
    "        kl_loss = -0.5*torch.sum(1 + rho - mu**2 - torch.exp(rho),axis=-1)\n",
    "        \n",
    "        loss = torch.mean(data_loss + kl_loss)\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        #break\n",
    "        #\n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        dl += torch.mean(data_loss).item()\n",
    "        kl += torch.mean(kl_loss).item()\n",
    "        n_batches += 1\n",
    "        \n",
    "        \n",
    "    print(dl/n_batches,kl/n_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mu,rho = encoder(X_test)\n",
    "\n",
    "z_intermediate = (mu + torch.randn_like(rho)*torch.exp(rho/2.)).detach().cpu().numpy()\n",
    "plt.scatter(z_intermediate[:,0],z_intermediate[:,1],c=y_test.detach().cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.xlabel('z_0')\n",
    "plt.ylabel('z_1')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(12,12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples per dimension\n",
    "n = 21\n",
    "\n",
    "# Sample between z=-2,2\n",
    "z_0 = np.linspace(-1,1,n)\n",
    "z_1 = np.linspace(-1,1,n)\n",
    "\n",
    "img_size = 28\n",
    "\n",
    "figure = np.zeros((img_size * n, img_size * n, 1))\n",
    "z_rest = [np.random.randn() for i in range(14)]\n",
    "\n",
    "for i,z0 in enumerate(z_0):\n",
    "    for j,z1 in enumerate(z_1):\n",
    "        z_sample = np.array([[z0,z1]])\n",
    "        x_decoded = decoder(torch.from_numpy(z_sample).to(device).to(torch.float))\n",
    "        img = x_decoded[0].reshape(img_size, img_size,1).cpu().detach().numpy()\n",
    "        figure[i * img_size: (i + 1) * img_size,j * img_size: (j + 1) * img_size,:] = img\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(figure.squeeze(),cmap=plt.cm.gray,origin='upper',extent=(-1,1,-1,1))\n",
    "\n",
    "#plt.scatter(z_intermediate[:,0],-z_intermediate[:,1],c=y_test.detach().cpu().numpy(),alpha=0.2)\n",
    "plt.xlim(-1,1)\n",
    "plt.ylim(-1,1)\n",
    "#plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "mu,rho = encoder(X_test)\n",
    "\n",
    "z_intermediate = (mu + torch.randn_like(rho)*torch.exp(rho/2.)).detach().cpu().numpy()\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(20,20)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "p = ax.scatter(z_intermediate[:,0],z_intermediate[:,1],z_intermediate[:,2],c=y_test.detach().cpu().numpy(),alpha=0.8)\n",
    "fig.colorbar(p)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "celeba_data = torchvision.datasets.ImageFolder('./img_align_celeba',transform=transformations)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = torch.utils.data.DataLoader(dataset=celeba_data,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 64\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        This method is where you'll want to instantiate parameters.\n",
    "        we do this by creating two linear transformation functions, l1 and l2, which \n",
    "        have encoded in it both the weight matrices W_1 and W_2, and the bias vectors\n",
    "        \"\"\"\n",
    "        super(Encoder,self).__init__()\n",
    "        self.conv_1 = nn.Conv2d(3,32,kernel_size=4,stride=2)\n",
    "        self.bn_1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(32,64,kernel_size=4,stride=2)\n",
    "        self.bn_2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv_3 = nn.Conv2d(64,128,kernel_size=4,stride=2)\n",
    "        self.bn_3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.conv_4 = nn.Conv2d(128,256,kernel_size=4,stride=2)\n",
    "        self.bn_4 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.fc_mu = nn.Linear(256*4*4,latent_dim)\n",
    "        self.fc_rho = nn.Linear(256*4*4,latent_dim)\n",
    "        \n",
    "        self.act = torch.nn.LeakyReLU()\n",
    "  \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        fmap_1 = self.act(self.bn_1(self.conv_1(F.pad(x,(1,2,1,2)))))\n",
    "        fmap_2 = self.act(self.bn_2(self.conv_2(F.pad(fmap_1,(1,2,1,2)))))\n",
    "        fmap_3 = self.act(self.bn_3(self.conv_3(F.pad(fmap_2,(1,2,1,2)))))\n",
    "        fmap_4 = self.act(self.bn_4(self.conv_4(F.pad(fmap_3,(1,2,1,2)))))\n",
    "        \n",
    "        fmap_flat = fmap_4.view(-1,256*4*4)\n",
    "        mu = self.fc_mu(fmap_flat)\n",
    "        rho = self.fc_rho(fmap_flat)\n",
    "        \n",
    "        return mu,rho\n",
    "        \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.fc = nn.Linear(latent_dim,256*4*4)\n",
    "        \n",
    "        self.upsample_1 = nn.Upsample((8,8))\n",
    "        self.conv_1 = nn.Conv2d(256,128,kernel_size=3,padding=1)\n",
    "        self.bn_1 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.upsample_2 = nn.Upsample((16,16))\n",
    "        self.conv_2 = nn.Conv2d(128,64,kernel_size=3,padding=1)\n",
    "        self.bn_2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.upsample_3 = nn.Upsample((32,32))\n",
    "        self.conv_3 = nn.Conv2d(64,32,kernel_size=3,padding=1)\n",
    "        self.bn_3 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.upsample_4 = nn.Upsample((64,64))\n",
    "        self.conv_4 = nn.Conv2d(32,3,kernel_size=3,padding=1)\n",
    " \n",
    "        self.act = torch.nn.LeakyReLU()\n",
    "  \n",
    "    def forward(self,z):\n",
    "        \"\"\"\n",
    "        This method runs the feedforward neural network.  It takes a tensor of size m x 784,\n",
    "        applies a linear transformation, applies a sigmoidal activation, applies the second linear transform \n",
    "        and outputs the logits.\n",
    "        \"\"\"\n",
    "        \n",
    "        fc = self.fc(z)\n",
    "        fc = fc.view(-1,256,4,4)\n",
    "        \n",
    "        #return self.conv_1(fc)\n",
    "        \n",
    "        fmap_1 = self.act(self.bn_1(self.conv_1(self.upsample_1(fc))))\n",
    "        fmap_2 = self.act(self.bn_2(self.conv_2(self.upsample_2(fmap_1))))\n",
    "        fmap_3 = self.act(self.bn_3(self.conv_3(self.upsample_3(fmap_2))))\n",
    "        fmap_4 = torch.sigmoid(self.conv_4(self.upsample_4(fmap_3)))\n",
    "        \n",
    "        return fmap_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()\n",
    "decoder = Decoder()\n",
    "\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam([e for e in encoder.parameters()]+[p for p in decoder.parameters()],lr=1e-3)\n",
    "\n",
    "sigma_data = 1.0\n",
    "epochs = 200\n",
    "# Loop over the data\n",
    "for epoch in range(epochs):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    # Loop over each subset of data\n",
    "    dl = 0\n",
    "    kl = 0\n",
    "    n_batches = 0\n",
    "    for d,_ in train_loader:\n",
    "        d = d.to(device)\n",
    "\n",
    "        # Zero out the optimizer's gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Make a prediction based on the model\n",
    "        mu, rho = encoder(d)\n",
    "        eps = torch.randn_like(rho).to(device)\n",
    "        latent = mu + 0.5*torch.exp(rho)*eps\n",
    "        reconstruction = decoder(latent)\n",
    "        \n",
    "        # Compute the loss\n",
    "        #data_loss = 0.5*torch.sum((reconstruction - d)**2/sigma_data**2,axis=-1)\n",
    "        data_loss = -0.5*torch.sum(d*torch.log(torch.clamp(reconstruction,min=1e-5)) + (1-d)*torch.log(torch.clamp(1-reconstruction,min=1e-5)),dim=(1,2,3))\n",
    "        kl_loss = -0.5*torch.sum(1 + rho - mu**2 - torch.exp(rho),axis=-1 )\n",
    "        \n",
    "        loss = torch.mean(data_loss + kl_loss)\n",
    "        # Use backpropagation to compute the derivative of the loss with respect to the parameters\n",
    "        loss.backward()\n",
    "        #break\n",
    "        #\n",
    "        # Use the derivative information to update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        dl += torch.mean(data_loss).item()\n",
    "        kl += torch.mean(kl_loss).item()\n",
    "        n_batches += 1\n",
    "        \n",
    "        if n_batches%20==0:\n",
    "           print('minibatch',n_batches,torch.mean(data_loss).item(),torch.mean(kl_loss).item())\n",
    "        \n",
    "        \n",
    "    print('epoch:',epoch,dl/n_batches,kl/n_batches)\n",
    "    torch.save(encoder.state_dict(), 'celeba_encoder_64.h5')\n",
    "    torch.save(decoder.state_dict(), 'celeba_decoder_64.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict(torch.load('celeba_encoder_64.h5'))\n",
    "decoder.load_state_dict(torch.load('celeba_decoder_64.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_state_dict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "idx = np.random.randint(d.cpu().numpy().shape[0])\n",
    "plt.imshow(np.moveaxis(d.cpu().numpy()[idx],0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.moveaxis(reconstruction.detach().cpu().numpy()[idx],0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "n = 5\n",
    "input_shape=[64,64]\n",
    "z_0 = np.linspace(-2,2,n)\n",
    "z_1 = np.linspace(-2,2,n)\n",
    "\n",
    "p = np.random.randint(64)\n",
    "q = np.random.randint(64)\n",
    "figure = np.zeros((input_shape[0] * n, input_shape[1] * n, 3))\n",
    "z_t = np.array([[np.random.randn()*0.0 for i in range(latent_dim)]])\n",
    "#z_t = np.array([[np.random.randn()*0.0 for i in range(32)]])\n",
    "\n",
    "print(p,q)\n",
    "for i,z0 in enumerate(z_0):\n",
    "    for j,z1 in enumerate(z_1):\n",
    "        z_sample = z_t.copy()\n",
    "        z_sample[0,p] = z0\n",
    "        z_sample[0,q] = z1\n",
    "        img = np.moveaxis(decoder(torch.tensor(z_sample).to(device).to(torch.float))[0].detach().cpu().numpy(),0,2)\n",
    "\n",
    "        figure[i * input_shape[0]: (i + 1) * input_shape[0],j * input_shape[1]: (j + 1) * input_shape[1],:] = img\n",
    "\n",
    "        \n",
    "#figure*=255\n",
    "#figure = figure.astype(int)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(figure.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_decoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
